{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5-txxuuqOs6"
   },
   "source": [
    "# Improve embedding with HuggingFace pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niRPdP0nqTIh",
    "outputId": "0f72f612-1009-499a-9c63-45009005495a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eQzUCtKqOs7"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "13HbexwlqOs8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44757/4234706728.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/home/rossnorman11/.pyenv/versions/3.10.6/envs/movie_recommendation_GPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#classic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#models\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nauIuYxTqOtB"
   },
   "source": [
    "## 1. Using pre-trained tiny-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ux1DfiOWqOtB"
   },
   "source": [
    "### 1. Text embedding with bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6m4gU9XHqOtB",
    "outputId": "1979be2c-e665-4803-f8ff-48ebc89c8990",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 14:21:54.792481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 14:21:54.792598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 14:21:54.793963: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 14:21:54.805776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 14:21:57.035857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-02-13 14:22:28.247225: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15627264 exceeds 10% of free system memory.\n",
      "2024-02-13 14:22:28.797801: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15627264 exceeds 10% of free system memory.\n",
      "2024-02-13 14:22:31.616650: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15627264 exceeds 10% of free system memory.\n",
      "2024-02-13 14:22:45.823622: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15627264 exceeds 10% of free system memory.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name, from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rossnorman11/.pyenv/versions/3.10.6/envs/movie_recommendation_GPT/lib/python3.10/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n",
      "2024-02-13 14:24:07.484301: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15627264 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "filename = 'bert.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Embed the (original) plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/shared_data/raw_data_shared/data_movie.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/shared_data/raw_data_shared/data_movie.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/movie_recommendation_GPT/lib/python3.10/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/movie_recommendation_GPT/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/shared_data/raw_data_shared/data_movie.pkl'"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_pickle('/content/drive/MyDrive/shared_data/raw_data_shared/data_movie.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soxq3DxP4En0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 262s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 14:28:07.459148: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1588224000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "plot = df['plot_synopsis'].tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "token_tensor = tokenizer(plot, padding='max_length', max_length= 500, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Create input tensors\n",
    "input_tensor = token_tensor['input_ids']\n",
    "\n",
    "# Generate embeddings\n",
    "prediction = model.predict(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bqxSRrc4ED6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=array([[[-0.53041697,  0.07894692, -4.1747184 , ...,  0.1208056 ,\n",
       "         -0.26163447,  1.4187179 ],\n",
       "        [-1.2223986 ,  0.2652639 ,  0.31511763, ..., -2.149884  ,\n",
       "         -0.3219931 ,  1.6733292 ],\n",
       "        [-0.84478337,  0.3376569 , -0.7529081 , ..., -2.9411268 ,\n",
       "          0.18474263,  0.29692876],\n",
       "        ...,\n",
       "        [-0.64974487,  0.19274138, -1.119652  , ..., -0.16245715,\n",
       "         -1.546545  ,  1.5404472 ],\n",
       "        [-0.05792686, -0.09606716, -0.3784339 , ..., -1.3717613 ,\n",
       "          0.20261516,  0.20651756],\n",
       "        [-0.73357034,  0.5554948 ,  0.4314581 , ..., -1.3273001 ,\n",
       "         -0.7229799 ,  0.76031506]],\n",
       "\n",
       "       [[-1.9052038 , -0.8651169 , -4.592863  , ..., -0.17127931,\n",
       "         -1.2534415 ,  1.5046825 ],\n",
       "        [-1.3443586 , -0.42073253, -0.6326513 , ..., -3.1596131 ,\n",
       "         -0.0952716 ,  0.46136558],\n",
       "        [-1.5785536 , -0.05970723, -0.8478688 , ..., -1.8083404 ,\n",
       "         -0.7600982 ,  1.138109  ],\n",
       "        ...,\n",
       "        [-2.0988564 , -0.58858293, -0.20193174, ..., -1.6806434 ,\n",
       "         -0.675322  , -0.4307701 ],\n",
       "        [-2.2840781 , -0.43265986, -0.30010036, ..., -0.5865475 ,\n",
       "          0.3722487 , -0.7887436 ],\n",
       "        [-2.7036061 , -0.60038424, -0.02198452, ..., -1.784806  ,\n",
       "         -0.63243455,  0.20079759]],\n",
       "\n",
       "       [[-0.28870422,  0.7776036 , -4.1618514 , ..., -0.11134764,\n",
       "         -1.0922282 ,  1.1931075 ],\n",
       "        [ 0.05996782,  0.265965  ,  0.3165695 , ..., -0.44653162,\n",
       "         -0.13188615,  1.3636253 ],\n",
       "        [-1.0561465 ,  0.98780125, -0.90692884, ..., -2.377137  ,\n",
       "         -0.5547937 ,  1.7435981 ],\n",
       "        ...,\n",
       "        [ 0.72139394,  0.47867167, -0.79339767, ..., -3.858223  ,\n",
       "         -0.13309565,  2.2026749 ],\n",
       "        [-0.28597218,  0.4807322 , -1.0973227 , ..., -2.0945299 ,\n",
       "         -1.1710443 ,  0.0287019 ],\n",
       "        [-1.1467263 ,  0.82778585, -1.5578413 , ..., -2.79878   ,\n",
       "         -0.66609806,  0.9152167 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.81509733, -0.68295026, -3.5512257 , ..., -0.33899134,\n",
       "         -1.1144799 ,  1.69915   ],\n",
       "        [-0.22656825, -0.89036644, -0.6034732 , ..., -3.431184  ,\n",
       "         -0.6521236 ,  0.9422919 ],\n",
       "        [-1.423491  , -0.78338844, -0.7949096 , ..., -2.513416  ,\n",
       "         -0.9287704 ,  1.4653559 ],\n",
       "        ...,\n",
       "        [-0.38809133, -0.73714936, -1.0686905 , ..., -3.156542  ,\n",
       "          0.11202541,  1.1203657 ],\n",
       "        [-0.37565792, -0.48932302, -0.52862024, ..., -1.5950423 ,\n",
       "         -1.371463  ,  0.6920314 ],\n",
       "        [-1.4063889 , -0.850213  , -0.15548228, ..., -1.418922  ,\n",
       "         -1.480375  ,  0.8113994 ]],\n",
       "\n",
       "       [[-0.03485826,  0.23413764, -4.033015  , ..., -0.37820655,\n",
       "         -0.8804451 ,  1.4345175 ],\n",
       "        [-0.60485053, -0.18081868, -0.59819716, ..., -3.0358357 ,\n",
       "         -0.4732022 ,  0.47032067],\n",
       "        [ 0.22060823,  0.6614527 , -0.37926936, ..., -1.0371168 ,\n",
       "         -1.6539665 ,  1.1674187 ],\n",
       "        ...,\n",
       "        [ 0.04853943,  0.31666234, -0.87226623, ..., -2.1281435 ,\n",
       "         -0.36667308,  1.011883  ],\n",
       "        [-0.91482663,  0.27356333, -0.82868344, ..., -2.0087938 ,\n",
       "         -1.2052684 , -0.41544783],\n",
       "        [-1.0918334 , -0.26310775,  0.22390598, ..., -1.9767019 ,\n",
       "         -1.7263243 , -0.6964369 ]],\n",
       "\n",
       "       [[ 0.48980227,  0.11469048, -4.8762474 , ..., -1.0247252 ,\n",
       "         -2.0596154 ,  0.5262044 ],\n",
       "        [-0.85291535, -0.03410205, -1.2711089 , ..., -1.9012676 ,\n",
       "         -0.91140413, -1.1701406 ],\n",
       "        [-0.7604286 ,  0.97647643, -0.5720707 , ..., -1.6879785 ,\n",
       "         -1.6952876 ,  0.36564973],\n",
       "        ...,\n",
       "        [ 0.7005044 ,  0.5632759 , -0.07127644, ..., -2.106988  ,\n",
       "         -0.7244139 ,  0.5779005 ],\n",
       "        [-0.30880514,  0.35611472, -0.7297251 , ..., -2.0276444 ,\n",
       "         -0.82829905,  0.897998  ],\n",
       "        [-0.9669198 ,  0.2556278 , -0.949358  , ..., -1.974671  ,\n",
       "         -1.1993691 ,  0.48279625]]], dtype=float32), pooler_output=array([[-0.92934054, -0.09118942, -0.9944727 , ..., -0.6296033 ,\n",
       "         0.9732787 ,  0.9681562 ],\n",
       "       [-0.9575624 , -0.10519944, -0.8143541 , ...,  0.9021431 ,\n",
       "         0.67696124,  0.9913458 ],\n",
       "       [-0.90079266, -0.13239166, -0.9694352 , ...,  0.6932387 ,\n",
       "         0.97142214,  0.98466253],\n",
       "       ...,\n",
       "       [-0.88811105, -0.11727145, -0.96163523, ...,  0.8651942 ,\n",
       "         0.9600147 ,  0.9765591 ],\n",
       "       [-0.9728537 , -0.15084106, -0.9501547 , ...,  0.7834512 ,\n",
       "         0.9532014 ,  0.98626167],\n",
       "       [-0.91367686, -0.07770363, -0.88730687, ...,  0.9875518 ,\n",
       "         0.96988046,  0.98223954]], dtype=float32), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Od-0mkqs4P3b"
   },
   "outputs": [],
   "source": [
    "# Process the embeddings as np\n",
    "plot_embeddings = prediction.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/content/drive/MyDrive/shared_data/processed_data_shared/embedding_plot.npy', plot_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Embed the (synthetic) summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv('/content/drive/MyDrive/shared_data/raw_data_shared/movie_with_summary.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = df['geny_summary'].tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "token_tensor = tokenizer(summaries, padding='max_length', max_length= 500, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Create input tensors\n",
    "input_tensor = token_tensor['input_ids']\n",
    "\n",
    "# Generate embeddings\n",
    "prediction = model.predict(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the embeddings as np\n",
    "summary_embeddings = prediction.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/content/drive/MyDrive/shared_data/processed_data_shared/embedding_summary.npy', summary_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLfIGWP84q5-"
   },
   "source": [
    "**Movie similarity with cosinus similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsJJRjXp4wZ5"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Calculate cosine similarity between embeddings\n",
    "# similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# # Example: Get top 10 most similar movies for each movie\n",
    "# num_movies = similarity_matrix.shape[0]\n",
    "# top_n = 1  # Number of similar movies to retrieve\n",
    "\n",
    "# for i in range(num_movies):\n",
    "#     # Sort similarity scores for movie i\n",
    "#     sim_scores = sorted(enumerate(similarity_matrix[i]), key=lambda x: x[1], reverse=True)\n",
    "#     # Exclude movie i itself\n",
    "#     sim_scores = sim_scores[1:]\n",
    "#     # Get indices of top similar movies\n",
    "#     top_indices = [idx for idx, _ in sim_scores[:top_n]]\n",
    "#     # Print movie titles of top similar movies\n",
    "#     print(f\"Top {top_n} similar movies for '{movie_titles[i]}':\")\n",
    "#     for idx in top_indices:\n",
    "#         print(f\"  - {movie_titles[idx]}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aozptTEv6BEK"
   },
   "source": [
    "**Movie recommendation with cosine similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "FgvA7AP-6Fk1",
    "outputId": "92104b06-f5c3-4c04-e2b1-cab766d96bcd"
   },
   "outputs": [],
   "source": [
    "# # Calculate Movie Similarity\n",
    "\n",
    "# similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# # Select User Preferences\n",
    "# user_input = input(\"Enter a movie title you like: \")\n",
    "\n",
    "# # Retrieve Similar Movies\n",
    "# movie_idx = movie_titles.index(user_input)\n",
    "# sim_scores = list(enumerate(similarity_matrix[movie_idx]))\n",
    "# sim_scores_sorted = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Filter Recommendations (optional)\n",
    "# top_n = 5  # Number of recommendations to display\n",
    "# recommended_movies = [movie_titles[idx] for idx, _ in sim_scores_sorted[1:top_n+1]]  # Exclude the input movie itself\n",
    "\n",
    "# # Display Recommendations\n",
    "# print(f\"Recommended movies based on '{user_input}':\")\n",
    "# for movie in recommended_movies:\n",
    "#     print(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nn0qCJV76PBW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
